# 🧠 Neural Network from Scratch using NumPy

This project demonstrates how to implement a **basic feedforward neural network** with **no machine learning frameworks**—only `NumPy`.

---

## 🧩 Problem
Solve the classic **XOR problem** using a multi-layer perceptron.

---

## 🔢 Architecture

- **Input Layer:** 2 neurons  
- **Hidden Layer:** 2 neurons with sigmoid activation  
- **Output Layer:** 1 neuron with sigmoid activation  

---

## ⚙️ Training Details

- Loss: Mean Squared Error (MSE)
- Optimizer: Gradient Descent (manual backpropagation)
- Epochs: 10,000
- Learning Rate: 0.1

---

## 🧠 Key Concepts

- Matrix multiplication for forward propagation  
- Sigmoid activation and its derivative  
- Manual calculation of gradients  
- Weight & bias updates with backpropagation  

---

## ▶️ Run the Code

```bash
python main.py
