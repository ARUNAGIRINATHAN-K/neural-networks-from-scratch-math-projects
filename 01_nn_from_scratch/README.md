# ğŸ§  Neural Network from Scratch using NumPy

This project demonstrates how to implement a **basic feedforward neural network** with **no machine learning frameworks**â€”only `NumPy`.

---

## ğŸ§© Problem
Solve the classic **XOR problem** using a multi-layer perceptron.

---

## ğŸ”¢ Architecture

- **Input Layer:** 2 neurons  
- **Hidden Layer:** 2 neurons with sigmoid activation  
- **Output Layer:** 1 neuron with sigmoid activation  

---

## âš™ï¸ Training Details

- Loss: Mean Squared Error (MSE)
- Optimizer: Gradient Descent (manual backpropagation)
- Epochs: 10,000
- Learning Rate: 0.1

---

## ğŸ§  Key Concepts

- Matrix multiplication for forward propagation  
- Sigmoid activation and its derivative  
- Manual calculation of gradients  
- Weight & bias updates with backpropagation  

---

## â–¶ï¸ Run the Code

```bash
python main.py
