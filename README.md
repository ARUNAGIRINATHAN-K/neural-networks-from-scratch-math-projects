# üß† Artificial Neural Network Projects (Mathematics + Implementation)

This repository showcases **10 curated projects** that demonstrate the mathematics behind Artificial Neural Networks (ANNs) using practical Python implementations. Ideal for learners, researchers, and aspiring data scientists.

---

## üîç Project List

| #  | Project Name                       | Description |
|----|-----------------------------------|-------------|
| 1  | [NN from Scratch](./01_nn_from_scratch)           | Fully connected ANN implemented using only NumPy (no ML libraries) |
| 2  | [XOR Classification](./02_xor_classification)     | Non-linear classification using a small neural network |
| 3  | [Digit Recognition - MNIST](./03_mnist_digit_recognition) | Classify handwritten digits using ANN |
| 4  | [NN Visualizer](./04_nn_visualizer)               | Visual tool to see activations, weights, and gradients |
| 5  | [Custom Dataset Classification](./05_custom_dataset_ann) | Train an ANN on a self-made feature-based dataset |
| 6  | [Loss Landscape](./06_loss_landscape)             | Visualize how loss changes with respect to weights |
| 7  | [Backpropagation Simulator](./07_backprop_simulator) | Simulate and explain backpropagation math |
| 8  | [Activation Function Comparison](./08_activation_function_analysis) | Study and compare Sigmoid, ReLU, Tanh |
| 9  | [Dropout Regularization](./09_dropout_regularization) | Manual implementation of dropout |
| 10 | [Time Series Forecasting](./10_time_series_ann)   | ANN for future value prediction from past data |

---

## üìå Key Concepts Covered
- Forward and backward propagation
- Weight updates using gradient descent
- Loss functions: MSE, Cross Entropy
- Activation functions
- Regularization techniques
- Visualization and interpretability

---

## ‚öôÔ∏è Tech Stack
- Python
- NumPy
- Matplotlib

---

## üí° How to Run
```bash
# Install dependencies
pip install -r requirements.txt

# Navigate to any project folder and run
cd 01_nn_from_scratch
python main.py
